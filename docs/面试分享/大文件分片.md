# 大文件分片上传

## 1. 实现思路

大文件分片上传的主要思路如下：

1. **文件切片**：使用 Blob.slice() 方法将大文件切分成若干个小块
2. **并发上传**：使用 Promise.all 控制并发上传切片
3. **断点续传**：将切片信息保存在 localStorage，支持断点续传
4. **文件秒传**：计算文件 hash，判断文件是否已上传
5. **切片合并**：所有切片上传完成后，通知服务端合并文件

## 2. 代码实现

### 2.1 文件切片实现

```javascript
//  以5M进行分片处理
function createFileChunk(file, size = 5 * 1024 * 1024) {
  const chunks = [];
  let cur = 0;
  while (cur < file.size) {
    chunks.push(file.slice(cur, cur + size));
    cur += size;
  }
  return chunks;
}
```

### 2.2 上传实现

```javascript
async function uploadChunks(chunks) {
  const requests = chunks.map((chunk, index) => {
    const formData = new FormData();
    formData.append('chunk', chunk);
    formData.append('hash', hash);
    formData.append('filename', filename);
    formData.append('chunkIndex', index);
    
    return axios.post('/upload', formData);
  });
  
  await Promise.all(requests);
}
```

### 2.3 断点续传实现

```javascript
function saveChunkProgress(filename, chunks) {
  localStorage.setItem(
    `${filename}-progress`,
    JSON.stringify(chunks.map((chunk, index) => ({
      index,
      size: chunk.size
    })))
  );
}

function getChunkProgress(filename) {
  return JSON.parse(localStorage.getItem(`${filename}-progress`)) || [];
}
```

### 2.4 文件秒传

```javascript
async function calculateHash(file) {
  return new Promise(resolve => {
    const reader = new FileReader();
    reader.readAsArrayBuffer(file);
    reader.onload = e => {
      const spark = new SparkMD5.ArrayBuffer();
      spark.append(e.target.result);
      resolve(spark.end());
    };
  });
}
```

## 3. 总结

大文件分片上传的核心要点：

1. **性能优化**：
   - 通过合理的切片大小提高上传效率
   - 控制并发数量避免服务器压力过大
   
2. **用户体验**：
   - 支持断点续传
   - 显示上传进度
   - 支持文件秒传
   
3. **可靠性保证**：
   - 使用 hash 校验文件完整性
   - 失败重试机制
   - 临时文件清理

4. **注意事项**：
   - 切片大小需要和服务端配合
   - 并发数要根据实际情况调整
   - 要考虑服务器存储临时文件的清理策略

通过以上方案，可以实现一个功能完善的大文件分片上传功能，既保证了上传的可靠性，又提供了良好的用户体验。